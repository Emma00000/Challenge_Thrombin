# -*- coding: utf-8 -*-
"""BúsquedaHiperparámetros_ML_Thrombin2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ce8t5oLJ_8AHlv3lNwgQA1RCL4TXXrbT

## Búsqueda de hiperparámetros
"""

import numpy as np
from collections import Counter
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report, make_scorer

from sklearn.model_selection import RandomizedSearchCV
import scipy

# Función del análisis de rendimiento 
def analysis(Ypred, testY):
    print('\nAccuracy: {}\n'.format(accuracy_score(Ypred, testY)))
    print('Precision: {}\n'.format(precision_score(Ypred, testY, average = 'weighted')))
    print('Recall: {}\n'.format(recall_score(Ypred, testY, average = 'weighted')))
    print('F-score: {}\n'.format(f1_score(Ypred, testY, average = 'weighted')))
    print('\nConfusion matrix: \n')
    print(str(confusion_matrix(Ypred, testY)) + '\n')
    print('Classification report: \n')
    print(classification_report(Ypred, testY, target_names = ['A', 'I']) + '\n')

# Cargar el datos obtenidos de análisis anteriores para entrenamiento
with open('./Thrombin/SMOTE-Xtrain_Chi2.csv', 'r') as file:
  train_set = file.readlines()
print('Se recuperaron: ', str(len(train_set)), ' líneas en el vector de SMOTE-Chi2.')

with open('./Thrombin/SMOTE-Xtrain_Chi2.csv', 'r') as file:
  train_class = file.readlines()
print('Se recuperaron: ', str(len(train_class)), ' clases en el vector de SMOTE.')

# Cargar datos de prueba con reducción
with open('./Thrombin/Xtest_Chi2.csv', 'r') as file:
  test_set = file.readlines()
print('Se recuperaron: ', str(len(test_set)), ' líneas en el vector de prueba Chi2.')

# Cargar el data set de prueba
with open('./Thrombin/ThrombinKey', 'r') as file:
  test_class = file.readlines()
print('Se recuperaron: ', str(len(test_class)), ' clases en el vector de prueba.')

# Guardar datos
train_data = []

for lines in train_set:
  lines = lines.replace("\n","")
  l = lines.split(",")
  l = [int(i) for i in l]
  train_data.append(l)
train_class = [i.replace("\n","") for i in train_class]

test_data = []

for lines in test_set:
  lines = lines.replace("\n","")
  l = lines.split(",")
  l = [int(i) for i in l]
  test_data.append(l)
test_class = [i.replace("\n","") for i in test_class]

train_data = np.array(train_data)
test_data = np.array(test_data)

print("\nDatos de entretamiento: ")
print(len(train_data),"  ",len(train_class))

print("\nDatos de prueba: ")
print(len(test_data),"  ",len(test_class))

"""# Búsqueda en balanceados y con reducción"""

print('\n\n\nBÚSQUEDA EN DATOS BALANCEADOS CON KERNEL LINEAL: \n')

# Búsqueda de hiperparámetros con cross-validation, SVC linear, 80 iteraciones y métrica con f1_score ponderada
param_clf = RandomizedSearchCV(cv = 3, estimator = SVC(probability = True, random_state = 19), 
                   n_iter = 80, n_jobs = -1, 
                   param_distributions = {'C': scipy.stats.expon(scale = 100), 
                                          'gamma': scipy.stats.expon(scale = .1),
                                          'kernel': ['linear']}
                               , random_state = 19, 
                   scoring = make_scorer(f1_score))

# Entrenar el modelo  
param_clf.fit(train_data, train_class)

Ypred = param_clf.predict(test_data)

analysis(Ypred, test_class)

