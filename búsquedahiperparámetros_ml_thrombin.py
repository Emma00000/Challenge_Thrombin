# -*- coding: utf-8 -*-
"""BúsquedaHiperparámetros_ML_Thrombin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ON0-b9MDFm4RtaSzlA2fc0h8Fa1jDnPv

## Búsqueda de hiperparámetros
"""

import numpy as np
from collections import Counter
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix,classification_report, make_scorer

from sklearn.feature_selection import chi2, SelectKBest, mutual_info_classif
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_selection import RFE

from imblearn.under_sampling import NearMiss, ClusterCentroids, EditedNearestNeighbours, AllKNN
from imblearn.over_sampling import SMOTE, ADASYN

from sklearn.model_selection import RandomizedSearchCV
import scipy

# Función del análisis de rendimiento 
def analysis(Ypred, testY):
    print('\nAccuracy: {}\n'.format(accuracy_score(Ypred, testY)))
    print('Precision: {}\n'.format(precision_score(Ypred, testY, average = 'weighted')))
    print('Recall: {}\n'.format(recall_score(Ypred, testY, average = 'weighted')))
    print('F-score: {}\n'.format(f1_score(Ypred, testY, average = 'weighted')))
    print('\nConfusion matrix: \n')
    print(str(confusion_matrix(Ypred, testY)) + '\n')
    print('Classification report: \n')
    print(classification_report(Ypred, testY, target_names = ['A', 'I']) + '\n')

# Cargar el data set de entrenamiento
with open('./Thrombin/thrombin.data', 'r') as file:
  train_set = file.readlines()
print('Se recuperaron: ', str(len(train_set)), ' líneas en el vector de entrenamiento.')


#Cargamos el data set de prueba
with open('./Thrombin-2/Thrombin.testset', 'r') as file:
  test_set = file.readlines()
print('Se recuperaron: ', str(len(test_set)), ' líneas en el vector de prueba.')

with open('./Thrombin/ThrombinKey', 'r') as file:
  test_class = file.readlines()
print('Se recuperaron: ', str(len(test_class)), ' clases en el vector de prueba.')

# Guardar datos 

train_data = []
train_class = []

for lines in train_set:
  lines = lines.replace("\n","")
  l = lines.split(",")
  train_class.append(l[0])
  l = l[1:]
  l = [int(i) for i in l]
  train_data.append(l)
    
train_data = np.array(train_data)
print("Datos de entretamiento: ")
print(len(train_data),"  ",len(train_class))

test_data=[]

for lines in test_set:
  lines = lines.replace("\n","")
  l = lines.split(",")[1:]
  l = [int(i) for i in l]
  test_data.append(l)
test_class = [i.replace("\n","") for i in test_class]

test_data = np.array(test_data)
print('Datos de test: ')
print(len(test_data),"  ",len(test_class))

# Verificar proporciones
print('Proporciones en los datos:')
print(sorted(Counter(train_class).items()))
print(sorted(Counter(test_class).items()))

"""# Búsqueda en balanceados y con reducción"""

# SMOTE 

print('\nSMOTE: \n')
x_resampled_SMOTE, y_resampled_SMOTE = SMOTE().fit_resample(train_data, train_class)
print(sorted(Counter(y_resampled_SMOTE).items()))

# Reducción de dimensión con Chi2
print('\nReducción de dimensión con Chi2: \n')

reducChi = SelectKBest(chi2, k = 700)
print(train_data.shape)
Xtrain_Chi2 = reducChi.fit_transform(x_resampled_SMOTE, y_resampled_SMOTE)
Xtest_Chi2 = reducChi.transform(test_data)
print(Xtrain_Chi2.shape)

print('\n\n\nBÚSQUEDA EN DATOS BALANCEADOS CON KERNEL LINEAL: \n')

# Búsqueda de hiperparámetros con cross-validation, SVC linear, 80 iteraciones y métrica con f1_score ponderada
param_clf = RandomizedSearchCV(cv = 3, estimator = SVC(probability = True, random_state = 19), 
                   n_iter = 80, n_jobs = -1, 
                   param_distributions = {'C': scipy.stats.expon(scale = 100), 
                                          'gamma': scipy.stats.expon(scale = .1),
                                          'kernel': ['linear']}
                               , random_state = 19, 
                   scoring = make_scorer(f1_score))

# Entrenar el modelo  
param_clf.fit(Xtrain_Chi2, y_resampled_SMOTE)

Ypred = param_clf.predict(Xtest_Chi2)

analysis(Ypred, test_class)



"""# Búsqueda en originales """

print('\n\n\nBÚSQUEDA EN DATOS ORIGINALES CON KERNEL LINEAL: \n')

# Búsqueda de hiperparámetros con cross-validation, SVC linear, 80 iteraciones y métrica con f1_score ponderada
param_clf = RandomizedSearchCV(cv = 3, estimator = SVC(probability = True, random_state = 19), 
                   n_iter = 80, n_jobs = -1, 
                   param_distributions = {'C': scipy.stats.expon(scale = 100), 
                                          'gamma': scipy.stats.expon(scale = .1),
                                          'kernel': ['linear'], 
                                          'class_weight':['balanced', None]}
                               , random_state = 19, 
                   scoring = make_scorer(f1_score))

# Entrenar el modelo para datos originales
param_clf.fit(train_data, train_class)

Ypred = param_clf.predict(test_data)

analysis(Ypred, test_class)

print('\n\n\nBÚSQUEDA EN DATOS ORIGINALES CON KERNEL RBF: \n')

# Búsqueda de hiperparámetros con cross-validation, SVC rbf, 80 iteraciones y métrica con f1_score ponderada
param_clf = RandomizedSearchCV(cv = 3, estimator = SVC(probability = True, random_state = 19), 
                   n_iter = 80, n_jobs = -1, 
                   param_distributions = {'C': scipy.stats.expon(scale = 100), 
                                          'gamma': scipy.stats.expon(scale = .1),
                                          'kernel': ['rbf'], 
                                          'class_weight':['balanced', None]}
                               , random_state = 19, 
                   scoring = make_scorer(f1_score))

# Entrenar el modelo para datos originales
param_clf.fit(train_data, train_class)

Ypred = param_clf.predict(test_data)

analysis(Ypred, test_class)

